---
title: "MCPâ€‘Eval Documentation"
description: "The comprehensive testing framework for MCP servers and tool-using agents. Build confidence in your AI integrations with production-grade evaluation tools."
icon: "house"
sidebarTitle: "Home"
mode: "custom"
---

<div style={{textAlign:'center', marginTop: 32, marginBottom: 32}}>
  <h1>MCPâ€‘Eval</h1>
  <p style={{fontSize: '1.25rem', marginBottom: 24}}>Your flight simulator for toolâ€‘using LLMs</p>
  <p style={{fontSize: '1rem', opacity: 0.9}}>Connect agents to real MCP servers â€¢ Run realistic scenarios â€¢ Assert behavior with confidence</p>
</div>

<Note>
  **New to MCP?** The Model Context Protocol enables AI assistants to connect with external tools and data sources. MCPâ€‘Eval ensures these connections work reliably in production.
</Note>

## Get started in 30 seconds

```bash
# Install MCP-Eval
pip install mcp-eval

# Initialize your project
mcp-eval init

# Run your first test
mcp-eval run examples/fetch_test.py
```

That's it! You're ready to start testing. [Continue with the Quickstart â†’](./quickstart.mdx)

## What MCPâ€‘Eval does for you

<CardGroup cols={2}>
  <Card title="Test MCP Servers" icon="server">
    Ensure your MCP servers respond correctly to agent requests and handle edge cases gracefully
  </Card>
  <Card title="Evaluate Agents" icon="robot">
    Measure how effectively agents use tools, follow instructions, and recover from errors
  </Card>
  <Card title="Track Performance" icon="chart-line">
    Monitor latency, token usage, cost, and success rates with OpenTelemetry-backed metrics
  </Card>
  <Card title="Assert Quality" icon="check-circle">
    Use structural checks, LLM judges, and path efficiency validators to ensure high quality
  </Card>
</CardGroup>

## Why teams choose MCPâ€‘Eval

* **Production-ready**: Built on OpenTelemetry for enterprise-grade observability
* **Multiple test styles**: Choose between decorators, pytest, or dataset-driven testing
* **Rich assertions**: Content checks, tool verification, performance gates, and LLM judges
* **CI/CD friendly**: GitHub Actions support, JSON/HTML reports, and regression detection
* **Language agnostic**: Test MCP servers written in any language

## Quick navigation

<CardGroup cols={3}>
  <Card title="Quickstart" icon="rocket" href="./quickstart">
    Get up and running in 5 minutes
  </Card>
  <Card title="Common Workflows" icon="graduation-cap" href="./common-workflows">
    Step-by-step guides for typical tasks
  </Card>
  <Card title="API Reference" icon="code" href="./api-catalog">
    Complete assertion catalog and APIs
  </Card>
</CardGroup>

## Learning path

### ðŸš€ Getting Started
* [Overview](./overview.mdx) - Understand MCPâ€‘Eval's architecture and philosophy
* [Quickstart](./quickstart.mdx) - Your first test in 5 minutes
* [Concepts](./concepts.mdx) - Core concepts and terminology

### ðŸ§ª Writing Tests
* [Assertions](./assertions.mdx) - The unified Expect API for all assertions
* [Common Workflows](./common-workflows.mdx) - Practical testing patterns
* [Test Generation](./test-generation.mdx) - AI-powered test creation

### ðŸ“Š Evaluation Types
* [Server Evaluation](./server-evaluation.mdx) - Testing MCP server implementations
* [Agent Evaluation](./agent-evaluation.mdx) - Measuring agent effectiveness
* [Datasets](./datasets.mdx) - Systematic evaluation suites

### ðŸ”§ Configuration & Integration
* [Configuration](./configuration.mdx) - Settings and customization
* [CI/CD](./ci-cd.mdx) - GitHub Actions and automation
* [Reports](./reports.mdx) - Understanding test outputs

### ðŸ“š Reference
* [CLI Reference](./cli-reference.mdx) - Complete command documentation
* [API Reference](./api-catalog.mdx) - Detailed API documentation
* [Troubleshooting](./troubleshooting.mdx) - Common issues and solutions
* [FAQ](./faq.mdx) - Frequently asked questions

## Example: Your first test

```python
from mcp_eval import task, Expect

@task("Verify fetch server works correctly")
async def test_fetch(agent, session):
    # Ask the agent to fetch a webpage
    response = await agent.generate_str("Fetch https://example.com and summarize it")
    
    # Assert the right tool was called
    await session.assert_that(Expect.tools.was_called("fetch"))
    
    # Verify the content is correct
    await session.assert_that(Expect.content.contains("Example Domain"), response=response)
    
    # Check performance
    await session.assert_that(Expect.performance.response_time_under(5000))
```

[See more examples â†’](./examples.mdx)

## Join the community

<CardGroup cols={2}>
  <Card title="GitHub" icon="github" href="https://github.com/anthropics/mcp-eval">
    Report issues and contribute
  </Card>
  <Card title="Discord" icon="discord" href="https://discord.gg/mcp-eval">
    Get help and share experiences
  </Card>
</CardGroup>

