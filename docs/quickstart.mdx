---
title: "Quickstart Guide"
description: "Get MCP‑Eval up and running in 5 minutes. Learn to install, configure, and run your first tests for MCP servers and agents."
sidebarTitle: "Quickstart"
icon: "rocket"
keywords: ["quickstart","installation","getting started","first test","mcp.json","setup"]
---

> Welcome to MCP‑Eval! This guide will have you testing MCP servers and agents in just 5 minutes.

## Before you begin

Make sure you have:

* **Python 3.10+** installed ([Download Python](https://www.python.org/downloads/))
* **An MCP server** to test (we'll use the fetch example)
* **An API key** for Claude or OpenAI (for LLM judge features)

<Tip>
  **New to MCP?** Start with the [MCP documentation](https://modelcontextprotocol.io) to understand the basics of Model Context Protocol servers.
</Tip>

## Get started in 5 steps (fetch example)

<Steps>
  <Step title="Install mcp-eval and set keys">
    ```bash
    pip install -e .
    # or
    uv pip install -e .
    ```

    Recommended judges/generation provider:

    ```bash
    export ANTHROPIC_API_KEY=...  # Claude Sonnet / Opus are ideal
    ```
  </Step>

  <Step title="Initialize the project">
    ```bash
    mcp-eval init
    ```

    - Writes `mcpeval.yaml` and secrets
    - Lets you import servers and set a default agent
    <!-- TODO(screenshot): init prompts and resulting files. -->
  </Step>

  <Step title="Add the fetch server">
    From `mcp.json`:
    ```bash
    mcp-eval add server --from-mcp-json .cursor/mcp.json
    ```
    Or from DXT:
    ```bash
    mcp-eval add server --from-dxt path/to/manifest.dxt
    ```
    See [Connect Servers](./connect-servers.mdx) for options.
  </Step>

  <Step title="Run the fetch tests">
    ```bash
    mcp-eval run examples/mcp_server_fetch/tests/test_decorator_style.py \
      -v --markdown test-reports/results.md --html test-reports/index.html
    ```
    Expected: `fetch` tool is called, output contains "Example Domain", and path efficiency passes.
    <!-- TODO(screenshot): terminal run summary and a passing HTML report section. -->
  </Step>

  <Step title="Inspect reports and iterate">
    - Open `test-reports/index.html`
    - Review assertion failures (e.g., output_matches), judge scores, and tool sequences
    - Tweak assertions or prompts, rerun
    <!-- TODO(screenshot): failed assertion detail view with diff. -->
  </Step>
</Steps>

## Detailed setup

### 1) Install

```bash
uv pip install -e .
# or
pip install -e .
```

Set provider keys (recommended for judges/generation):

```bash
export ANTHROPIC_API_KEY=...   # recommended for generation/judging
export OPENAI_API_KEY=...
```

### 2) Initialize a project

```bash
mcp-eval init
```

- Captures provider/model (uses ModelSelector if not given)
- Writes `mcpeval.yaml` and `mcpeval.secrets.yaml`
- Lets you add/import a server and define a default agent

Source: [Generator CLI](../src/mcp_eval/cli/generator.py)

### 3) Add a server

Interactive:

```bash
mcp-eval add server
```

From `mcp.json` (Cursor/VS Code integrations):

```bash
mcp-eval add server --from-mcp-json .cursor/mcp.json
```

From DXT manifest (Anthropic Desktop):

```bash
mcp-eval add server --from-dxt path/to/manifest.dxt
```

References:
- `mcp.json` format: [mcp.json guide](https://gofastmcp.com/integrations/mcp-json-configuration)
- DXT: [DXT README](https://github.com/anthropics/dxt/blob/main/README.md)

### 4) Define an agent

During init you set a default agent (AgentSpec). You can add more:

```bash
mcp-eval add agent
```

Source: [Generator CLI](../src/mcp_eval/cli/generator.py), [CLI utils](../src/mcp_eval/cli/utils.py)

### 5) Run tests

- Decorator tests:

```bash
mcp-eval run examples/mcp_server_fetch/tests/test_decorator_style.py -v --markdown out.md --html out.html
```

- Pytest style:

```bash
pytest examples/mcp_server_fetch/tests/test_pytest_style.py -v
```

- Dataset file:

```bash
mcp-eval run dataset --file examples/mcp_server_fetch/datasets/basic_fetch_dataset.yaml
```

Runner source: [Runner](../src/mcp_eval/runner.py)

### 6) Generate tests with an LLM (optional)

We recommend Anthropic Sonnet/Opus for better generation quality.

```bash
mcp-eval generate --style pytest --n-examples 8 --provider anthropic
```

This discovers tools, drafts scenarios, refines assertions, and writes tests.

Source: [Generator CLI](../src/mcp_eval/cli/generator.py)

### Reports and traces

- JSON/Markdown/HTML combined reports
- Per‑test JSON and OTEL `.jsonl` traces under `./test-reports`

Configure in `mcpeval.yaml` ([schema](../schema/mcpeval.config.schema.json)), see [Configuration](../src/mcp_eval/config.py).

#### Files created by init

- `mcpeval.yaml` for eval knobs and agents
- `mcpeval.secrets.yaml` for API keys
- Updates to your mcp‑agent settings if needed

#### Minimal test (decorator)

```python
from mcp_eval import task, Expect

@task("Fetch and verify")
async def test_fetch(agent, session):
  resp = await agent.generate_str("Fetch https://example.com")
  await session.assert_that(Expect.tools.was_called("fetch"), response=resp)
  await session.assert_that(Expect.content.contains("Example Domain"), response=resp)
```

<!-- TODO: Add screenshot of init prompts; add screenshot of first run summary; add screenshot of reports directory contents. -->


